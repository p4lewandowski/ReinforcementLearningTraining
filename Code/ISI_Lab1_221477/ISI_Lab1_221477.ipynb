{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Zadanie-1---Ocena-strategii-(Policy-evaluation)\" data-toc-modified-id=\"Zadanie-1---Ocena-strategii-(Policy-evaluation)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Zadanie 1 - Ocena strategii (<em>Policy evaluation</em>)</a></span></li><li><span><a href=\"#Zadanie-2---Iteracyjne-doskonalenie-strategii-(Policy-iteration)\" data-toc-modified-id=\"Zadanie-2---Iteracyjne-doskonalenie-strategii-(Policy-iteration)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Zadanie 2 - Iteracyjne doskonalenie strategii (<em>Policy iteration</em>)</a></span></li><li><span><a href=\"#Zadanie-3---Iteracyjne-obliczanie-funkcji-wartości-(Value-iteration)\" data-toc-modified-id=\"Zadanie-3---Iteracyjne-obliczanie-funkcji-wartości-(Value-iteration)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Zadanie 3 - Iteracyjne obliczanie funkcji wartości (<em>Value iteration</em>)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGNzaFkLphFz"
   },
   "source": [
    "# Laboratorium 1\n",
    "\n",
    "Celem pierwszego laboratorium jest zapoznanie się oraz zaimplementowanie algorytmów uczenia pasywnego. Zaimplementowane algorytmy będą testowane z wykorzystaniem wcześniej przygotowanego środowiska przedstawionego na schemacie poniżej:\n",
    "![MDP, Markov Decision Process](assets/images/mdp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0FsLU4FphF2"
   },
   "source": [
    "Dołączenie biblioteki ze środowiskiem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1512,
     "status": "error",
     "timestamp": 1583780227457,
     "user": {
      "displayName": "Przemysław Lewandowski",
      "photoUrl": "",
      "userId": "13164902765317467134"
     },
     "user_tz": -60
    },
    "id": "lKOAONPHphF4",
    "outputId": "58ef4a43-fd03-4b1a-c76f-aeb56bd30ced"
   },
   "outputs": [],
   "source": [
    "from env.simpleMDP import simpleMDP\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rs3fTWzHphF-"
   },
   "source": [
    "Zapoznanie się z przygotowanym środowiskiem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ILv-tN8rphGA",
    "outputId": "9d76f086-06c1-4396-f298-cb501b55cbac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s0', 's1', 's2']\n",
      "State: s0 action: a0 list of possible next states:  {'s0': 0.5, 's2': 0.5}\n",
      "State: s0 action: a1 list of possible next states:  {'s2': 1}\n",
      "State: s1 action: a0 list of possible next states:  {'s0': 0.7, 's1': 0.1, 's2': 0.2}\n",
      "State: s1 action: a1 list of possible next states:  {'s1': 0.95, 's2': 0.05}\n",
      "State: s2 action: a0 list of possible next states:  {'s0': 0.4, 's2': 0.6}\n",
      "State: s2 action: a1 list of possible next states:  {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n"
     ]
    }
   ],
   "source": [
    "mdp = simpleMDP()\n",
    "\n",
    "states = mdp.get_all_states()\n",
    "print(states)\n",
    "\n",
    "for s in states:\n",
    "    actions = mdp.get_possible_actions(s)\n",
    "    for a in actions:\n",
    "        next_states = mdp.get_next_states(s, a)\n",
    "        print(\"State: \" + s + \" action: \" + a + \" \" + \"list of possible next states: \", next_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XRQ-_7V7phGH"
   },
   "source": [
    "## Zadanie 1 - Ocena strategii (*Policy evaluation*)\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Celem ćwiczenie jest zaimplementowanie algorytmu oceny strategii. Algorytm wyznacza funkcję oceny (wartości) strategii wykorzystując równanie Bellmana. Odbywa się to w sposób iteracyjny zgodnie ze wzorem:\n",
    "\\begin{equation}\n",
    "\t        V_{k + 1}(s) = \\sum_a \\pi(a | s) \\sum_{s'} \\sum_r p(s', r|s, a)[r + \\gamma V_k(s')]\n",
    "\\end{equation}\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "npEErL4iphGK"
   },
   "source": [
    "Pierwszym krokiem jest zainicjalizowanie strategii, która będzie podlegała ocenie. W tym zadaniu ocenie będzie podlegała strategia stochastyczna, w której będzie prawdopodobieństwo wyboru każdej z możliwych opcji będzie takie samo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UfJqX_T2phGM",
    "outputId": "79d9a60c-4790-42ae-e5b5-b1ed70b94b1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s0': {'a0': 0.5, 'a1': 0.5}, 's1': {'a0': 0.5, 'a1': 0.5}, 's2': {'a0': 0.5, 'a1': 0.5}}\n"
     ]
    }
   ],
   "source": [
    "policy = dict()\n",
    "\n",
    "for s in states:\n",
    "    actions = mdp.get_possible_actions(s)\n",
    "    action_prob = 1 / len(actions)\n",
    "    policy[s] = dict()\n",
    "    for a in actions:\n",
    "        policy[s][a] = action_prob\n",
    "\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8hqNPxS6phGU"
   },
   "source": [
    "Implementacja algorytmu oceny strategii - podejście z dwiema tablicami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zMFpsTyUphGX"
   },
   "outputs": [],
   "source": [
    "def policy_eval_two_arrays(mdp, policy, gamma, theta):\n",
    "    \"\"\"\n",
    "    This function uses the two-array approach to evaluate the specified policy for the specified MDP:\n",
    "\n",
    "    'mdp' - model of the environment, use following functions:\n",
    "        get_all_states - return list of all states available in the environment\n",
    "        get_possible_actions - return list of possible actions for the given state\n",
    "        get_next_states - return list of possible next states with a probability for transition from state by taking\n",
    "                          action into next_state\n",
    "\n",
    "    'policy' - the stochastic policy (action probability for each state), for the given mdp, too evaluate\n",
    "    'gamma' - discount factor for MDP\n",
    "    'theta' - algorithm should stop when minimal difference between previous evaluation of policy and current is smaller\n",
    "              than theta\n",
    "    \"\"\"\n",
    "    V = {s: random.random() for s in mdp.get_all_states()}\n",
    "    V_check = {s: 0 for s in mdp.get_all_states()}\n",
    "    \n",
    "    while True:\n",
    "        V_check = V.copy()\n",
    "        delta = 0\n",
    "        for s in mdp.get_all_states():\n",
    "            v = 0\n",
    "            for a in mdp.get_possible_actions(s):\n",
    "                for s_next, s_next_probability in mdp.get_next_states(s, a).items():\n",
    "                    v += policy[s][a] * s_next_probability * (mdp.get_reward(s, a, s_next) + gamma * V_check[s_next])\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "                                                                       \n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xVEvi83rphGb"
   },
   "source": [
    "Sprawdzenie poprawności zaimplementowanego algorytmu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rDWE7C-jphGd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yay!\n"
     ]
    }
   ],
   "source": [
    "V = policy_eval_two_arrays(mdp, policy, 0.9, 0.0001)\n",
    "\n",
    "assert np.isclose(V['s0'], 1.46785443374683)\n",
    "assert np.isclose(V['s1'], 4.55336594491180)\n",
    "assert np.isclose(V['s2'], 1.68544141660991)\n",
    "\n",
    "print(\"Yay!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1dp96DtRphGj"
   },
   "source": [
    "Implementacja algorytmu oceny strategii - obliczenia w miejscu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GjO1qL2pphGk"
   },
   "outputs": [],
   "source": [
    "def policy_eval_in_place(mdp, policy, gamma, theta):\n",
    "    \"\"\"\n",
    "    This function uses the in-place approach to evaluate the specified policy for the specified MDP:\n",
    "\n",
    "    'mdp' - model of the environment, use following functions:\n",
    "        get_all_states - return list of all states available in the environment\n",
    "        get_possible_actions - return list of possible actions for the given state\n",
    "        get_next_states - return list of possible next states with a probability for transition from state by taking\n",
    "                          action into next_state\n",
    "\n",
    "    'policy' - the stochastic policy (action probability for each state), for the given mdp, too evaluate.\n",
    "    'gamma' - discount factor for MDP\n",
    "    'theta' - algorithm should stop when minimal difference between previous evaluation of policy and current is smaller\n",
    "              than theta\n",
    "    \"\"\"\n",
    "    V = {s: 0 for s in mdp.get_all_states()}\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in mdp.get_all_states():\n",
    "            v = 0\n",
    "            for a in mdp.get_possible_actions(s):\n",
    "                for s_next, s_next_probability in mdp.get_next_states(s, a).items():\n",
    "                    v += policy[s][a] * s_next_probability * (mdp.get_reward(s, a, s_next) + gamma * V[s_next])\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lhTx2UGIphGr"
   },
   "source": [
    "Sprawdzenie poprawności zaimplementowanego algorytmu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XEZOGSVKphGs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yay!\n"
     ]
    }
   ],
   "source": [
    "V = policy_eval_in_place(mdp, policy, 0.9, 0.0001)\n",
    "\n",
    "assert np.isclose(V['s0'], 1.4681508097651)\n",
    "assert np.isclose(V['s1'], 4.5536768132712)\n",
    "assert np.isclose(V['s2'], 1.6857723431614)\n",
    "\n",
    "print(\"Yay!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lHpxPjCYphGy"
   },
   "source": [
    "## Zadanie 2 - Iteracyjne doskonalenie strategii (*Policy iteration*)\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Celem ćwiczenia jest zaimplementowanie algorytmu iteracyjnego doskonalenia strategii. Zadaniem algorytmu jest wyznaczenie optymalnej strategii dla danego środowiska. Pierwszym krokiem algorytmu jest zainicjalizowanie strategii losowymi akcjami, następnie jej ocenienie oraz poprawa, zgodnie ze wzorem:\n",
    "\\begin{equation}\n",
    "    \\pi(s) \\leftarrow  \\text{argmax}_a \\sum_{s'} \\sum_r p(s', r|s, a)[r + \\gamma V(s')]\n",
    "\\end{equation}\n",
    "Jeżeli poprawiona strategia będzie się różniła od poprzedniej, należy ją ponownie ocenić i poprawić. Algorytm kończy działanie w momencie kiedy poprawiona strategia będzie dokładnie taka sama, jak ta poddawana poprawie.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z-7KXZTYphGz"
   },
   "source": [
    "Zaimplementuj funkcję do oceny strategii, tylko tym razem przyjmij, że strategia przekazywana na wejściu będzie deteministyczna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s0': {'a0': 1, 'a1': 0}, 's1': {'a0': 1, 'a1': 0}, 's2': {'a0': 1, 'a1': 0}}\n"
     ]
    }
   ],
   "source": [
    "policy = dict()\n",
    "\n",
    "for s in states:\n",
    "    actions = mdp.get_possible_actions(s)\n",
    "    action_prob = 1\n",
    "    policy[s] = dict()\n",
    "    for a in actions:\n",
    "        policy[s][a] = action_prob\n",
    "        action_prob = 0\n",
    "\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "70OccJo9phG0"
   },
   "outputs": [],
   "source": [
    "def deterministic_policy_eval_in_place(mdp, policy, gamma, theta):\n",
    "    \"\"\"\n",
    "    This function uses the in-place approach to evaluate the specified deterministic policy for the specified MDP:\n",
    "\n",
    "        'mdp' - model of the environment, use following functions:\n",
    "        get_all_states - return list of all states available in the environment\n",
    "        get_possible_actions - return list of possible actions for the given state\n",
    "        get_next_states - return list of possible next states with a probability for transition from state by taking\n",
    "                          action into next_state\n",
    "\n",
    "        'policy' - the deterministic policy (action probability for each state), for the given mdp, too evaluate\n",
    "        'gamma' - discount factor for MDP\n",
    "        'theta' - algorithm should stop when minimal difference between previous evaluation of policy and current is\n",
    "                  smaller than theta\n",
    "    \"\"\"\n",
    "    V = dict()\n",
    "\n",
    "    for s in mdp.get_all_states():\n",
    "        V[s] = 0\n",
    " \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in mdp.get_all_states():\n",
    "            v = 0\n",
    "            for a in mdp.get_possible_actions(s):\n",
    "                for s_next, s_next_probability in mdp.get_next_states(s, a).items():\n",
    "                    v += policy[s][a] * s_next_probability * (mdp.get_reward(s, a, s_next) + gamma * V[s_next])\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HWinI20_phG6"
   },
   "source": [
    "Zaimplementuj funkcję do poprawy strategii `policy` na podstawie funkcji oceny `value_function` wyznaczonej dla niej "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AtDh4llAphG7"
   },
   "outputs": [],
   "source": [
    "def policy_improvement(mdp, policy, value_function, gamma):\n",
    "    \"\"\"\n",
    "            This function improves specified deterministic policy for the specified MDP using value_function:\n",
    "\n",
    "           'mdp' - model of the environment, use following functions:\n",
    "                get_all_states - return list of all states available in the environment\n",
    "                get_possible_actions - return list of possible actions for the given state\n",
    "                get_next_states - return list of possible next states with a probability for transition from state by taking\n",
    "                                  action into next_state\n",
    "\n",
    "           'policy' - the deterministic policy (action for each state), for the given mdp, too improve.\n",
    "           'value_function' - the value function, for the given policy.\n",
    "            'gamma' - discount factor for MDP\n",
    "\n",
    "           Function returns True if policy was improved or False otherwise\n",
    "       \"\"\"\n",
    "\n",
    "    policy_stable = True\n",
    "    out_policy = {s: {a: 0 for a in mdp.get_possible_actions(s)} for s in mdp.get_all_states()}\n",
    "\n",
    "    for s in mdp.get_all_states():\n",
    "        for a in mdp.get_possible_actions(s):\n",
    "            v = 0\n",
    "            for s_next, s_next_probability in mdp.get_next_states(s, a).items():\n",
    "                out_policy[s][a] += s_next_probability * (mdp.get_reward(s, a, s_next) + gamma * value_function[s_next])\n",
    "                \n",
    "    # reonstructing new policy\n",
    "    new_values = [np.max(list(s_dict.values())) for s_dict in out_policy.values()]\n",
    "    for i, s in enumerate(mdp.get_all_states()):\n",
    "        for a in mdp.get_possible_actions(s):\n",
    "            out_policy[s][a] = 1 if out_policy[s][a] == new_values[i] and 1 not in out_policy[s].values() else 0\n",
    "            \n",
    "    if out_policy != policy:\n",
    "        policy = out_policy\n",
    "        policy_stable = False\n",
    "\n",
    "    return policy_stable, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1EQuxlf-phHA"
   },
   "source": [
    "Zaimplementuj funkcję do iteracyjnego doskonalenia strategii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JmBTBEcvphHC"
   },
   "outputs": [],
   "source": [
    "def policy_iteration(mdp, gamma, theta):\n",
    "\n",
    "    \"\"\"\n",
    "            This function calculate optimal policy for the specified MDP:\n",
    "\n",
    "           'mdp' - model of the environment, use following functions:\n",
    "                get_all_states - return list of all states available in the environment\n",
    "                get_possible_actions - return list of possible actions for the given state\n",
    "                get_next_states - return list of possible next states with a probability for transition from state by taking\n",
    "                                  action into next_state\n",
    "\n",
    "           'gamma' - discount factor for MDP\n",
    "           'theta' - algorithm should stop when minimal difference between previous evaluation of policy and current is smaller\n",
    "                      than theta\n",
    "           Function returns optimal policy and value function for the policy\n",
    "       \"\"\"\n",
    "\n",
    "    policy = dict()\n",
    "\n",
    "    for s in states:\n",
    "        actions = mdp.get_possible_actions(s)\n",
    "        action_prob = 1\n",
    "        policy[s] = dict()\n",
    "        for a in actions:\n",
    "            policy[s][a] = action_prob\n",
    "            action_prob = 0\n",
    "\n",
    "    policy_stable = False\n",
    "\n",
    "    while not policy_stable:\n",
    "        V = deterministic_policy_eval_in_place(mdp, policy, gamma, theta)\n",
    "        policy_stable, new_policy = policy_improvement(mdp, policy, V, gamma)\n",
    "        \n",
    "        if not policy_stable:\n",
    "            V_new = deterministic_policy_eval_in_place(mdp, policy, gamma, theta)\n",
    "\n",
    "            new_values = [val for val in V_new.values()]\n",
    "            current_values = [val for val in V.values()]\n",
    "\n",
    "            if all([a >= b for a, b in zip(new_values, current_values)]):\n",
    "                policy = new_policy\n",
    "                V = V_new\n",
    "                \n",
    "    # Get best action for state s\n",
    "    actions = [max(x, key=x.get) for x in policy.values()]\n",
    "    for i, s in enumerate(mdp.get_all_states()):\n",
    "        policy[s] = actions[i]\n",
    "\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0gj-c00pphHH"
   },
   "source": [
    "Sprawdzenie poprawności zaimplementowanego algorytmu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oxfyVtENphHJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s0': 'a1', 's1': 'a0', 's2': 'a1'} {'s0': 3.785366128142998, 's1': 7.298653645273432, 's2': 4.206831790079636}\n",
      "Yay!\n"
     ]
    }
   ],
   "source": [
    "optimal_policy, optimal_value = policy_iteration(mdp, 0.9, 0.001)\n",
    "print(optimal_policy, optimal_value)\n",
    "\n",
    "assert optimal_policy['s0'] == 'a1'\n",
    "assert optimal_policy['s1'] == 'a0'\n",
    "assert optimal_policy['s2'] == 'a1'\n",
    "\n",
    "assert np.isclose(optimal_value['s0'], 3.78536612814300)\n",
    "assert np.isclose(optimal_value['s1'], 7.29865364527343)\n",
    "assert np.isclose(optimal_value['s2'], 4.20683179007964)\n",
    "\n",
    "print(\"Yay!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "op5gmF8TphHQ"
   },
   "source": [
    "## Zadanie 3 - Iteracyjne obliczanie funkcji wartości (*Value iteration*)\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Celem ćwiczenia jest zaimplementowanie algorytmu iteracyjnego obliczania funkcji wartości. Algorytm ten łączy w sobie dwa wyżej wspomniane podejścia oceny i wyznaczania optymalnej strategii. Najpierw wyznaczana jest optymalna funkcja wartości stanu zgodnie ze wzorem:\n",
    "\\begin{equation}\n",
    "    V(s) \\leftarrow  \\max_a \\sum_{s'} \\sum_r p(s', r|s, a)[r + \\gamma V(s')].\n",
    "\\end{equation}\n",
    "Po wyznaczeniu optymalnej funkcji wartości dla każdego stanu określana jest strategia postępowania w każdym możliwym stanie zgodnie ze wzorem:\n",
    "\\begin{equation}\n",
    "    \\pi(s) = \\text{argmax}_a \\sum_{s'} \\sum_r p(s', r|s, a)[r + \\gamma V(s')].\n",
    "\\end{equation}\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "64Su9VK7phHS"
   },
   "source": [
    "Implementacja algorytmu do iteracyjnego obliczania funkcji wartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fnQO0ZonphHU"
   },
   "outputs": [],
   "source": [
    "def value_iteration(mdp, gamma, theta):\n",
    "    \"\"\"\n",
    "            This function calculate optimal policy for the specified MDP using Value Iteration approach:\n",
    "\n",
    "            'mdp' - model of the environment, use following functions:\n",
    "                get_all_states - return list of all states available in the environment\n",
    "                get_possible_actions - return list of possible actions for the given state\n",
    "                get_next_states - return list of possible next states with a probability for transition from state by taking\n",
    "                                  action into next_state\n",
    "\n",
    "            'gamma' - discount factor for MDP\n",
    "            'theta' - algorithm should stop when minimal difference between previous evaluation of policy and current is\n",
    "                      smaller than theta\n",
    "            Function returns optimal policy and value function for the policy\n",
    "       \"\"\"\n",
    "    V = dict()\n",
    "    policy = dict()\n",
    "\n",
    "    # init with a policy with first avail action for each state\n",
    "    for current_state in mdp.get_all_states():\n",
    "        V[current_state] = 0\n",
    "        policy[current_state] = actions[0]\n",
    "                \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in mdp.get_all_states():\n",
    "            v = V[s]\n",
    "            V_s_a = {a: 0 for a in mdp.get_possible_actions(s)}\n",
    "            for a in mdp.get_possible_actions(s):\n",
    "                for s_next, s_next_probability in mdp.get_next_states(s, a).items():\n",
    "                    V_s_a[a] += s_next_probability * (mdp.get_reward(s, a, s_next) + gamma * V[s_next])\n",
    "            V[s] = max(list(V_s_a.values()))\n",
    "            delta = max(delta, np.abs(v - V[s])) # previous vs new\n",
    "            policy[s] = max(V_s_a.keys(), key=(lambda k: V_s_a[k]))\n",
    "                \n",
    "        if delta < theta:\n",
    "            break\n",
    "                    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xIyBNGG6phHZ"
   },
   "source": [
    "Sprawdzenie poprawności zaimplementowanego algorytmu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rLfMGWKlphHa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yay!\n"
     ]
    }
   ],
   "source": [
    "optimal_policy, optimal_value = value_iteration(mdp, 0.9, 0.001)\n",
    "\n",
    "assert optimal_policy['s0'] == 'a1'\n",
    "assert optimal_policy['s1'] == 'a0'\n",
    "assert optimal_policy['s2'] == 'a1'\n",
    "\n",
    "assert np.isclose(optimal_value['s0'], 3.78536612814300)\n",
    "assert np.isclose(optimal_value['s1'], 7.29865364527343)\n",
    "assert np.isclose(optimal_value['s2'], 4.20683179007964)\n",
    "\n",
    "print(\"Yay!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab. 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
