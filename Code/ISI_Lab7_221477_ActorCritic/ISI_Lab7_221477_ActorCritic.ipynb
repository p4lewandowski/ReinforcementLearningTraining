{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Zadanie-1---Actor-Critic\" data-toc-modified-id=\"Zadanie-1---Actor-Critic-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Zadanie 1 - Actor-Critic</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorium 7\n",
    "\n",
    "Celem siódmego laboratorium jest zapoznanie się oraz zaimplementowanie algorytmu głębokiego uczenia aktywnego - Actor-Critic. Zaimplementowany algorytm będzie testowany z wykorzystaniem środowiska z OpenAI - *CartPole*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dołączenie standardowych bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dołączenie bibliotek do obsługi sieci neuronowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1 - Actor-Critic\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Celem ćwiczenie jest zaimplementowanie algorytmu Actor-Critic. W tym celu należy utworzyć dwie głębokie sieci neuronowe:\n",
    "    1. *actor* - sieć, która będzie uczyła się optymalnej strategii (podobna do tej z laboratorium 6),\n",
    "    2. *critic* - sieć, która będzie uczyła się funkcji oceny stanu (podobnie jak się DQN).\n",
    "Wagi sieci *actor* aktualizowane są zgodnie ze wzorem:\n",
    "\\begin{equation*}\n",
    "    \\theta \\leftarrow \\theta + \\alpha \\delta_t \\nabla_\\theta log \\pi_{\\theta}(a_t, s_t | \\theta).\n",
    "\\end{equation*}\n",
    "Wagi sieci *critic* aktualizowane są zgodnie ze wzorem:\n",
    "\\begin{equation*}\n",
    "    w \\leftarrow w + \\beta \\delta_t \\nabla_w\\upsilon(s_{t + 1}, w),\n",
    "\\end{equation*}\n",
    "gdzie:\n",
    "\\begin{equation*}\n",
    "    \\delta_t \\leftarrow r_t + \\gamma \\upsilon(s_{t + 1}, w) - \\upsilon(s_t, w).\n",
    "\\end{equation*}\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACagent:\n",
    "    def __init__(self, action_size, actor, policy, critic):\n",
    "        self.action_size = action_size\n",
    "        self.gamma = 0.98    # discount rate\n",
    "        self.actor = actor\n",
    "        self.policy = policy\n",
    "        self.critic = critic #critic network should have only one output\n",
    "        self.possible_actions = [0, 1]\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the action to take in the current state, basing on policy returned by the network.\n",
    "\n",
    "        Note: To pick action according to the probability generated by the network\n",
    "        \"\"\"\n",
    "        possible_actions = self.possible_actions\n",
    "        action_probabilities = self.policy.predict(state[np.newaxis, :])[0]\n",
    "\n",
    "        # Pick possible action based on its probabilities from network\n",
    "        return np.random.choice(self.possible_actions, p = action_probabilities)\n",
    "\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Function learn networks using information about state, action, reward and next state. \n",
    "        First the values for state and next_state should be estimated based on output of critic network.\n",
    "        Critic network should be trained based on target value:\n",
    "        target = r + \\gamma next_state_value if not done]\n",
    "        target = r if done.\n",
    "        Actor network shpuld be trained based on delta value:\n",
    "        delta = target - state_value\n",
    "        \"\"\"\n",
    "        # Comply with batches\n",
    "        state = state[np.newaxis, :]\n",
    "        next_state = next_state[np.newaxis, :]\n",
    "    \n",
    "        # Calculate critic's opinion\n",
    "        critic_val = self.critic.predict(state)\n",
    "        critic_val_next = self.critic.predict(next_state)\n",
    "\n",
    "        # state value\n",
    "        target_val = reward + (done is not True)*self.gamma*critic_val_next\n",
    "        \n",
    "        # delta - how good is the state considering critic's opinion\n",
    "        delta = target_val - critic_val\n",
    "\n",
    "        # make one hot for selected actions - now one action only\n",
    "        actions_one_hot = np.zeros([1, self.action_size])\n",
    "        actions_one_hot[np.arange(1), action] = 1 \n",
    "        \n",
    "        # Train - actor to follow good policy, critic to evaluate target properly\n",
    "        self.actor.fit([state, delta], actions_one_hot, verbose=0) # definitly verbose = 0\n",
    "        self.critic.fit(state, target_val, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czas przygotować model sieci, która będzie się uczyła działania w środowisku [*CartPool*](https://gym.openai.com/envs/CartPole-v0/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\virnael\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\").env\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "alpha_learning_rate = 0.00015\n",
    "beta_learning_rate = 0.00055\n",
    "\n",
    "input_layer = Input(shape=(env.observation_space.shape[0], )) # for state tensor\n",
    "delta = Input(shape=[1])\n",
    "dense_1 = Dense(24, activation='relu')(input_layer)\n",
    "dense_2 = Dense(24, activation='relu')(dense_1)\n",
    "output_layer = Dense(env.action_space.n, activation='softmax')(dense_2)\n",
    "evaluation_layer = Dense(1, activation='linear')(dense_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_ascend(true, pred):\n",
    "    log_lk = true * K.log(K.clip(pred, 1e-8, 1)) # log of 0 is a bad idea\n",
    "    return K.mean(-log_lk * delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Actor\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"Critic\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 745\n",
      "Trainable params: 745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "actor = Model(inputs=[input_layer, delta], outputs=[output_layer], name='Actor')\n",
    "actor.compile(optimizer=Adam(lr=alpha_learning_rate), loss=gradient_ascend)\n",
    "\n",
    "critic = Model(inputs=[input_layer], outputs=[evaluation_layer], name='Critic')\n",
    "critic.compile(optimizer=Adam(lr=beta_learning_rate), loss='mean_squared_error')\n",
    "\n",
    "policy = Model(inputs=[input_layer], outputs=[output_layer])\n",
    "\n",
    "actor.summary()\n",
    "critic.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czas nauczyć agenta gry w środowisku *CartPool*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ACagent(action_size=env.action_space.n, actor=actor, policy = policy, critic=critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:13.380\n",
      "mean reward:14.140\n",
      "mean reward:27.000\n",
      "mean reward:87.260\n",
      "mean reward:174.890\n",
      "mean reward:236.400\n",
      "mean reward:435.390\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    score_history = []\n",
    "\n",
    "    for i in range(100):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.learn(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "        score_history.append(score)\n",
    "\n",
    "    print(\"mean reward:%.3f\" % (np.mean(score_history)))\n",
    "\n",
    "    if np.mean(score_history) > 300:\n",
    "        print(\"You Win!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make training faster I would definitly emply some buffer policy, otherwise there is no real parallelization in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
